{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T19:19:00.704686Z",
     "start_time": "2019-07-29T19:19:00.700811Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # so the IDs match nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # \"0, 1\" to select the desired GPU's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T19:19:01.925456Z",
     "start_time": "2019-07-29T19:19:00.712072Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'keras_export'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-84fcde209dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mskm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLocallyDirectedConnected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLocallyDirectedConnected\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mLocallyDirectedConnected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/GenNet/code/tensorflow2/LocallyDirectedConnected.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'keras_export'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import tables\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import sklearn.metrics as skm\n",
    "import LocallyDirectedConnected\n",
    "# when using tensorflow 2:\n",
    "# import tensorflow2.LocallyDirectedConnected as LocallyDirectedConnected\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy import stats\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.metrics import average_precision_score\n",
    "from utils import sensitivity, specificity, evaluate_performance\n",
    "tf.keras.backend.set_epsilon(0.0000001)\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred): \n",
    "    y_true = K.backend.clip(y_true, 0.0001, 1)\n",
    "    y_pred = K.backend.clip(y_pred, 0.0001, 1)\n",
    "\n",
    "    return K.backend.mean(-y_true * K.backend.log(y_pred + 0.0001) * weight_positive_class - (1 - y_true) * K.backend.log(\n",
    "        1 - y_pred + 0.0001) * weight_negative_class)\n",
    "\n",
    "\n",
    "def get_valdata(datapath):\n",
    "    yval = pd.read_csv(datapath + \"yval_\"+studyname+\".csv\")\n",
    "    h5file = tables.open_file(datapath  + studyname + '_genotype_processed.h5', \"r\")\n",
    "    ybatch = yval[\"labels\"]\n",
    "    xbatchid = np.array(yval[\"tot_index\"].values, dtype=np.int64)\n",
    "    xbatch = h5file.root.data[xbatchid, :]\n",
    "    ybatch = np.reshape(np.array(ybatch), (-1, 1))\n",
    "    h5file.close()\n",
    "    return (xbatch, ybatch)\n",
    "\n",
    "\n",
    "def get_testdata(datapath):\n",
    "    ytest = pd.read_csv(datapath + \"ytest_\"+studyname+\".csv\")\n",
    "    h5file = tables.open_file(datapath  + studyname + '_genotype_processed.h5', \"r\")\n",
    "    ybatch = ytest[\"labels\"]\n",
    "    xbatchid = np.array(ytest[\"tot_index\"].values, dtype=np.int64)\n",
    "    xbatch = h5file.root.data[xbatchid, :]\n",
    "    ybatch = np.reshape(np.array(ybatch), (-1, 1))\n",
    "    h5file.close()\n",
    "    return (xbatch, ybatch)\n",
    "\n",
    "\n",
    "class train_data_generator(K.utils.Sequence):\n",
    "\n",
    "    def __init__(self, datapath, batch_size, trainsize, startindex, stopindex, shuffle = True):\n",
    "        self.datapath = datapath\n",
    "        self.batch_size = batch_size\n",
    "        self.ytrainsize = trainsize\n",
    "        self.startindex = startindex\n",
    "        self.stopindex = stopindex\n",
    "        self.shuffledindexes = np.arange(trainsize)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.shuffledindexes)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ytrainsize / float(self.batch_size)))\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "            batchindexes = self.shuffledindexes[idx * self.batch_size:((idx + 1) * self.batch_size)]\n",
    "            ytrain = pd.read_csv(datapath + \"ytrain_\"+studyname+\".csv\")\n",
    "            h5file = tables.open_file(self.datapath + studyname + '_genotype_processed.h5', \"r\")\n",
    "            ybatch = ytrain[\"labels\"].iloc[batchindexes]\n",
    "            xbatchid = np.array(ytrain[\"tot_index\"].iloc[batchindexes], dtype=np.int64)\n",
    "            xbatch = h5file.root.data[xbatchid, :]\n",
    "            ybatch = np.reshape(np.array(ybatch), (-1, 1))\n",
    "            h5file.close()\n",
    "            return xbatch, ybatch\n",
    "    \n",
    "\n",
    "    \n",
    "    def on_epoch_begin(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        np.random.shuffle(self.shuffledindexes)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        np.random.shuffle(self.shuffledindexes)\n",
    "        \n",
    "        \n",
    "class val_data_generator(K.utils.Sequence):\n",
    "\n",
    "    def __init__(self, datapath, batch_size, valsize, startindex, stopindex):\n",
    "        self.datapath = datapath\n",
    "        self.batch_size = batch_size\n",
    "        self.yvalsize = valsize\n",
    "        self.startindex = startindex\n",
    "        self.stopindex = stopindex\n",
    "\n",
    "    def __len__(self):\n",
    "        val_len = int(np.floor(self.yvalsize / float(self.batch_size)))\n",
    "        return val_len \n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        yval = pd.read_csv(self.datapath + \"yval_\"+studyname+\".csv\")\n",
    "        h5file = tables.open_file(self.datapath + studyname + '_genotype_processed.h5', \"r\")\n",
    "        ybatch = yval[\"labels\"].iloc[idx * self.batch_size:((idx + 1) * self.batch_size)]\n",
    "        xbatchid = np.array(yval[\"tot_index\"].iloc[idx * self.batch_size:((idx + 1) * self.batch_size)], dtype=np.int64)\n",
    "        xbatch = h5file.root.data[xbatchid,:]\n",
    "        ybatch = np.reshape(np.array(ybatch), (-1, 1))\n",
    "        h5file.close()\n",
    "        return xbatch, ybatch\n",
    "\n",
    "\n",
    "def Lasso(inputsize, l1_value):\n",
    "    inputs = K.Input((inputsize,), name='inputs')\n",
    "    x1 = K.layers.BatchNormalization(center=False, scale=False, name=\"inter_out\")(inputs)\n",
    "    x1 = K.layers.Dense(units=1, kernel_regularizer=K.regularizers.l1(l1value))(x1)\n",
    "    x1 = K.layers.Activation(\"sigmoid\")(x1)\n",
    "    model = K.Model(inputs=inputs, outputs=x1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def GenNet_gene_layer(inputsize):\n",
    "    mask = scipy.sparse.load_npz( datapath + '/SNP_gene_mask.npz')\n",
    "    \n",
    "    input_ = K.Input((inputsize,), name='input_layer')\n",
    "    input_layer = K.layers.Reshape(input_shape=(inputsize,), target_shape=(inputsize, 1))(input_)\n",
    "    \n",
    "    gene_layer = LocallyDirectedConnected.LocallyDirected1D(mask=mask, filters=1, input_shape=(inputsize, 1), name=\"gene_layer\")(input_layer)\n",
    "    gene_layer = K.layers.Flatten()(gene_layer)\n",
    "    gene_layer = K.layers.Activation(\"tanh\")(gene_layer)\n",
    "    gene_layer = K.layers.BatchNormalization(center=False, scale=False, name=\"inter_out\")(gene_layer)\n",
    "\n",
    "    output_layer = K.layers.Dense(units=1, name=\"output_layer\")(gene_layer)\n",
    "    output_layer = K.layers.Activation(\"sigmoid\")(output_layer)\n",
    "\n",
    "    model = K.Model(inputs=input_, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "def GenNet_gene_layer_l1(inputsize, l1_value = 0.01):\n",
    "    mask = scipy.sparse.load_npz( datapath + '/SNP_gene_mask.npz')\n",
    "    \n",
    "    input_ = K.Input((inputsize,), name='input_layer')\n",
    "    input_layer = K.layers.Reshape(input_shape=(inputsize,), target_shape=(inputsize, 1))(input_)\n",
    "    \n",
    "    gene_layer = LocallyDirectedConnected.LocallyDirected1D(mask=mask, filters=1,\n",
    "                                                            input_shape=(inputsize, 1),\n",
    "                                                            name=\"gene_layer\",\n",
    "                                                            activity_regularizer=tf.keras.regularizers.l1(l=0.01))(input_layer)\n",
    "    gene_layer = K.layers.Flatten()(gene_layer)\n",
    "    gene_layer = K.layers.Activation(\"tanh\")(gene_layer)\n",
    "    gene_layer = K.layers.BatchNormalization(center=False, scale=False, name=\"inter_out\")(gene_layer)\n",
    "\n",
    "    output_layer = K.layers.Dense(units=1, name=\"output_layer\",activity_regularizer=tf.keras.regularizers.l1(l=0.01),\n",
    "                                  kernel_regularizer = tf.keras.regularizers.l1(l=l1_value) )(gene_layer)\n",
    "    output_layer = K.layers.Activation(\"sigmoid\")(output_layer)\n",
    "\n",
    "    model = K.Model(inputs=input_, outputs=output_layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-29T19:19:00.708Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jobid = 1\n",
    "modeltype = \"GenNet_gene_layer\"\n",
    "\n",
    "optimizer = \"Adadelta\" # good to start with\n",
    "batch_size = 32\n",
    "\n",
    "namescore = 'score' + str(jobid)\n",
    "\n",
    "basepath = os.getcwd()[:-4]\n",
    "datapath = basepath + \"/processed_data/\"\n",
    "studyname = str(np.load(datapath + \"studyname.npy\"))\n",
    "\n",
    "epochs = 200\n",
    "l1_value = 0.01\n",
    "weight_positive_class = 3 # adjust for imbalanced datasets\n",
    "weight_negative_class = 1\n",
    "\n",
    "print(studyname)\n",
    "print(weight_positive_class)\n",
    "print(weight_negative_class)\n",
    "\n",
    "train_size = len(pd.read_csv(datapath + \"ytrain_\"+studyname+\".csv\"))\n",
    "val_size = len(pd.read_csv(datapath + \"yval_\"+studyname+\".csv\"))\n",
    "\n",
    "\n",
    "if optimizer == \"Adam\":\n",
    "    lr_opt = 0.0006 # seems to work in most cases\n",
    "    optimizer = tf.keras.optimizers.Adam(lr = lr_opt)\n",
    "if optimizer == \"Adadelta\":\n",
    "    optimizer = tf.keras.optimizers.Adadelta()\n",
    "    \n",
    "\n",
    "folder = (str(studyname)+ \"__\" +str(jobid) )\n",
    "\n",
    "h5file = tables.open_file(datapath  + studyname + '_genotype_processed.h5', \"r\")\n",
    "data_shape = h5file.root.data.shape\n",
    "inputsize = h5file.root.data.shape[1]\n",
    "startindex = 0\n",
    "stopindex= -1\n",
    "h5file.close()\n",
    "\n",
    "rfrun_path = \"//media/avanhilten/pHDD1TB/SCZ/results/\" + folder + \"/\"\n",
    "if not os.path.exists(rfrun_path):\n",
    "    print(\"Runpath did not exist but is made now\")\n",
    "    os.mkdir(rfrun_path)\n",
    "\n",
    "print(\"jobid =  \" + str(jobid))\n",
    "print(\"folder = \" + str(folder))\n",
    "print(\"batchsize = \" + str(batch_size))\n",
    "print(\"n_features \" + str(inputsize))\n",
    "\n",
    "if modeltype == \"GenNet_gene_layer\":\n",
    "    model = GenNet_gene_layer(inputsize=int(inputsize))    \n",
    "if modeltype == \"GenNet_gene_layer_l1\":\n",
    "    model = GenNet_gene_layer_l1(inputsize=int(inputsize))    \n",
    "if modeltype == \"Lasso\":\n",
    "    model = Lasso(inputsize=int(inputsize), l1_value = l1_value)  \n",
    "    \n",
    "model.compile(loss=weighted_binary_crossentropy, optimizer=optimizer, metrics=[\"accuracy\",sensitivity, specificity])\n",
    "\n",
    "print(model.summary())\n",
    "model_summary = str(model.to_json())\n",
    "\n",
    "with open(rfrun_path + '/experiment_stats_results_.txt', 'a') as f:\n",
    "    f.write('gtname = ' + str(studyname))\n",
    "    f.write('\\n jobid = ' + str(jobid))\n",
    "    f.write('\\n model = ' + str(modeltype))\n",
    "    f.write('\\n batchsize = ' + str(batch_size))\n",
    "    f.write('\\n weightnegative = ' + str(weight_negative_class))\n",
    "    f.write('\\n weightpositive = ' + str(weight_positive_class))\n",
    "\n",
    "with open(rfrun_path + '/experiment_summary_model.txt', 'w') as fh:\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "csv_logger = K.callbacks.CSVLogger(rfrun_path + 'log.csv', append=True, separator=';')\n",
    "earlystop =K.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=25, verbose=1, mode='auto')\n",
    "saveBestModel = K.callbacks.ModelCheckpoint(rfrun_path + \"bestweight_job.h5\", monitor='val_loss',\n",
    "                                          verbose=1, save_best_only=True, mode='auto')\n",
    "reduce_lr = K.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "# %%\n",
    "if os.path.exists(rfrun_path + '/bestweight_job.h5'):\n",
    "    print('loading weights')\n",
    "    model.load_weights(rfrun_path + '/bestweight_job.h5')\n",
    "    print(\"evaluate over \" + str(data_shape[0]) + \" patients\")\n",
    "    xval, yval = get_valdata(datapath)\n",
    "    evaluate_val = model.evaluate(xval, yval)\n",
    "else:\n",
    "    history = model.fit_generator(generator=train_data_generator(datapath=datapath, batch_size=batch_size, trainsize=int(train_size),\n",
    "                                                        startindex=startindex, stopindex=stopindex),\n",
    "                        shuffle=True,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlystop, saveBestModel, csv_logger, reduce_lr],\n",
    "                        workers=5,\n",
    "                        use_multiprocessing=True,\n",
    "                        validation_data=val_data_generator(datapath=datapath, batch_size=batch_size, valsize=val_size,\n",
    "                                                            startindex=startindex, stopindex=stopindex)\n",
    "                        )\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Finished\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-29T19:19:00.709Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"evaluate over patients with the model from the latest epoch\")\n",
    "xval, yval = get_valdata(datapath)\n",
    "pval = model.predict(xval)\n",
    "evaluate_performance(yval, pval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-29T19:19:00.711Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('loading weights')\n",
    "model.load_weights(rfrun_path + '/bestweight_job.h5')\n",
    "print(\"evaluate over the patients with the best model\")\n",
    "pval = model.predict(xval)\n",
    "evaluate_performance(yval, pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-29T19:19:00.714Z"
    }
   },
   "outputs": [],
   "source": [
    "xtest, ytest = get_testdata(datapath)\n",
    "ptest = model.predict(xtest)\n",
    "evaluate_performance(ytest, ptest)\n",
    "np.save(rfrun_path + \"/ptest.npy\", ptest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#7dcfe2','#4b78b5','darkgrey','dimgray','#7dcfe2','#4b78b5','darkgrey','dimgray','#7dcfe2','#4b78b5','darkgrey','dimgray','#7dcfe2','#4b78b5','darkgrey','dimgray','#7dcfe2','#4b78b5','darkgrey','dimgray','#7dcfe2','#4b78b5','darkgrey','dimgray']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "num_annotated = 10\n",
    "gene_end  = np.load(datapath + \"gene_end.npy\")\n",
    "gene_middle = []\n",
    "for i in  range(len(gene_end) - 1):\n",
    "    gene_middle.append((gene_end[i]+ gene_end[i+1])/2)\n",
    "\n",
    "\n",
    "\n",
    "y = model.layers[6].get_weights()[0]\n",
    "\n",
    "y = y / max(y)\n",
    "x = np.arange(len(y))\n",
    "\n",
    "plt.ylim(bottom=-1.2, top=1.19)\n",
    "plt.xlim(0, len(x) + 100)\n",
    "\n",
    "plt.xlabel(\"Genes (colored per chromosome)\", size=24)\n",
    "plt.ylabel(\"Weights\", size=24)\n",
    "\n",
    "gene_overview = pd.read_csv(datapath + \"/gene_overview.csv\", sep = \"\\t\")\n",
    "gene_overview['mean'] = y\n",
    "gene_overview = gene_overview.sort_values([\"CHR\", \"bp\"], ascending = (True, True))\n",
    "gene_overview[\"pos\"] = x\n",
    "\n",
    "    \n",
    "x = gene_overview[\"pos\"].values\n",
    "y = gene_overview[\"mean\"].values\n",
    "\n",
    "for i in range(len(gene_end) - 1):\n",
    "    plt.scatter(x[gene_end[i]:gene_end[i + 1]], y[gene_end[i]:gene_end[i + 1]],  c=colors[i])\n",
    "    \n",
    "gene_overview_annotate = gene_overview.sort_values(\"mean\", ascending=False).head(num_annotated)\n",
    "top_hits_genes = gene_overview.sort_values(\"mean\", ascending=False).copy()\n",
    "\n",
    "\n",
    "for i in range(num_annotated):\n",
    "    plt.annotate(gene_overview_annotate[\"gene\"].iloc[i],\n",
    "                 (gene_overview_annotate[\"pos\"].iloc[i], gene_overview_annotate[\"mean\"].iloc[i]),\n",
    "                 xytext=(gene_overview_annotate[\"pos\"].iloc[i] + 100,\n",
    "                         gene_overview_annotate[\"mean\"].iloc[i] + np.random.randint(100) * 0.0006), size=16)\n",
    "\n",
    "\n",
    "plt.ylim(bottom = 0)\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.bottom'] = True\n",
    "plt.rcParams['axes.spines.left'] = True\n",
    "plt.yticks( size=16)\n",
    "plt.xticks(gene_middle, np.arange(len(gene_middle))+1, size=16)\n",
    "plt.savefig(datapath + 'Manhattan_genes_blue.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-29T19:19:00.717Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "ytrain = np.reshape(np.array(pd.read_csv(datapath + \"ytrain_\"+studyname+\".csv\")[\"labels\"]), (-1, 1))\n",
    "ptrain = np.squeeze(model.predict_generator(train_data_generator(datapath=datapath, batch_size=100, trainsize=int(train_size),\n",
    "                                                startindex=startindex, stopindex=stopindex, shuffle = False), workers=8, use_multiprocessing=True ))\n",
    "\n",
    "np.save(rfrun_path + \"/ptrain.npy\", ptrain)\n",
    "\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(name='inter_out').output)\n",
    "\n",
    "intermediate_layer_model.compile(loss=weighted_binary_crossentropy, optimizer=optimizer,\n",
    "                                 metrics=[\"accuracy\", sensitivity, specificity])\n",
    "\n",
    "intermediate_output = np.squeeze(intermediate_layer_model.predict_generator(train_data_generator(datapath=datapath, batch_size=100, trainsize=int(train_size),\n",
    "                                               startindex=startindex, stopindex=stopindex, shuffle = False), workers=8, use_multiprocessing=True ))\n",
    "\n",
    "\n",
    "\n",
    "np.save(rfrun_path + \"/intermediate_output.npy\", intermediate_output)\n",
    "norm_mean = np.mean(intermediate_output, axis=0)\n",
    "norm_std = np.std(intermediate_output, axis=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(norm_mean)\n",
    "plt.xlabel(\"mean\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.title(\"hist mean of activations\")\n",
    "plt.savefig(rfrun_path + \"mean_act_hist\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(norm_std)\n",
    "plt.xlabel(\"std\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.title(\"hist std of activations\")\n",
    "plt.savefig(rfrun_path + \"std_act_hist\")\n",
    "plt.show()\n",
    "\n",
    "np.save(rfrun_path + \"/ptrain.npy\", ptrain)\n",
    "print(\"mean ptrain = \" + str(np.mean(ptrain)))\n",
    "print(\"min ptrain = \" + str(np.min(ptrain)))\n",
    "print(\"max ptrain = \" + str(np.max(ptrain)))\n",
    "\n",
    "print(\"\\n f1 = \" + str(skm.f1_score(ytrain, ptrain.round())))\n",
    "print(\"\\n confusion matrix\")\n",
    "cm = skm.confusion_matrix(ytrain, ptrain.round())\n",
    "print(cm)\n",
    "ptrain.max()\n",
    "\n",
    "fpr, tpr, thresholds = skm.roc_curve(ytrain, ptrain)\n",
    "roc_auc = skm.auc(fpr, tpr)\n",
    "\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(optimal_threshold)\n",
    "\n",
    "lw = 2\n",
    "Specificity1 = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "print('Specificity : ', Specificity1)\n",
    "\n",
    "Sensitivity1 = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "print('Sensitivity : ', Sensitivity1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.plot(1 - Specificity1, Sensitivity1, color='b', marker='o')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate / Recall')\n",
    "plt.ylabel('True Positive Rate / Precision')\n",
    "plt.title('AUCROC and Precision-Recall curve:')\n",
    "\n",
    "print(\"\\n train: loss, acc, sensitivity, specificity\")\n",
    "# print(evaluate_train)\n",
    "print(\"\\n AUC = \" + str(roc_auc) + '\\n')\n",
    "print(skm.classification_report(y_pred=np.round(ptrain), y_true=ytrain))\n",
    "\n",
    "with open(rfrun_path + '/experiment_stats_results_.txt', 'a') as f:\n",
    "    f.write(str(cm))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n mean ptrain = \" + str(np.mean(ptrain)))\n",
    "    f.write(\"\\n f1 = \" + str(skm.f1_score(ytrain, ptrain.round())))\n",
    "    f.write(\"\\n auc = \" + str(roc_auc))\n",
    "\n",
    "average_precision = average_precision_score(ytrain, ptrain)\n",
    "precision, recall, thresholds = precision_recall_curve(ytrain, ptrain)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post', label='Average Precision (area = {0:0.2f})'.format(\n",
    "        average_precision))\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.savefig(rfrun_path + studyname + \"_ROCtrain.png\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "data = h5py.File(rfrun_path + \"/bestweight_job.h5\", \"r\")\n",
    "dname = [s for s in [key for key in data[\"model_weights\"].keys()] if \"output\" in s]\n",
    "weights_dense = np.array(data[\"model_weights\"][dname[0]][dname[0]][\"kernel:0\"]).flatten()\n",
    "data.close()\n",
    "\n",
    "\n",
    "x= np.arange(len(weights_dense))\n",
    "\n",
    "results_fold = pd.DataFrame(np.array([weights_dense, x]).T, columns=['weights_dense', \"pos\"])\n",
    "results_fold[\"norm_mean\"]  = norm_mean\n",
    "results_fold[\"norm_std\"] = norm_std\n",
    "results_fold.to_csv(rfrun_path + \"results_fold.csv\")\n",
    "\n",
    "compare_weight_stdcor = results_fold.weights_dense.values * results_fold.norm_std.values\n",
    "\n",
    "x = np.arange(len(compare_weight_stdcor))\n",
    "compare_weight_stdcor = np.abs(compare_weight_stdcor) / np.max(np.abs(compare_weight_stdcor))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "num_annotated = 10\n",
    "gene_end = np.load(datapath + \"gene_end.npy\")\n",
    "\n",
    "for i in range(len(gene_end) - 1):\n",
    "    plt.scatter(x[gene_end[i]:gene_end[i + 1]], compare_weight_stdcor[gene_end[i]:gene_end[i + 1]], c=colors[i])\n",
    "        \n",
    "plt.ylim(bottom= 0 , top=1.2)\n",
    "plt.xlim(0, len(compare_weight_stdcor )+ 100)\n",
    "plt.title(\"Gene Importance\", size=36)\n",
    "plt.xlabel(\"Gene coordinate (colored per chromosome)\", size=18)\n",
    "plt.ylabel(\"Importance \", size=18)\n",
    "\n",
    "gene_overview = pd.read_csv(datapath + \"gene_overview.csv\", sep = \"\\t\")\n",
    "gene_overview['mean'] = compare_weight_stdcor\n",
    "gene_overview[\"pos\"] = x\n",
    "gene5_overview = gene_overview.sort_values(\"mean\", ascending=False).head(num_annotated)\n",
    "top_hits_genes = gene_overview.sort_values(\"mean\", ascending=False).copy()\n",
    "\n",
    "\n",
    "for i in range(num_annotated):\n",
    "    plt.annotate(gene5_overview[\"gene\"].iloc[i],\n",
    "                 (gene5_overview[\"pos\"].iloc[i], gene5_overview[\"mean\"].iloc[i]),\n",
    "                 xytext=(gene5_overview[\"pos\"].iloc[i] + 100,\n",
    "                         gene5_overview[\"mean\"].iloc[i]), size=16)\n",
    "\n",
    "gene5_overview = gene_overview.sort_values(\"mean\", ascending=True).head(num_annotated)\n",
    "\n",
    "\n",
    "plt.gca().spines['right'].set_color('none')\n",
    "plt.gca().spines['top'].set_color('none')\n",
    "\n",
    "plt.savefig(rfrun_path + \"geneimportance_train_10f_c_wide.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
